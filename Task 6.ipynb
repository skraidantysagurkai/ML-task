{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5608c45c-7087-4f0f-ab99-e442460c32ba",
   "metadata": {},
   "source": [
    "# Task 6\n",
    "**In this task we will create a predictor using scikit to predict GDP per capita. We will be exluding other GDP related data fields.**\n",
    "\\\n",
    "\\\n",
    "Minitasks: \\\n",
    " a) We will show prediction error (MSE) on the training and the testing data sets.\\\n",
    " b) Name the fields we will use.\\\n",
    " c) Find the top 5 fields/features that contribute the most to te predictions.\\\n",
    " d) Train another predictor that uses those top 5 features.\\\n",
    " e) Save the predictor in a file.\\\n",
    "For this task I am thinking of using scikit's other_dataGBOOST as the data will be straight forward.\n",
    "\\\n",
    "\\\n",
    "Steps:\n",
    "1) We will first preprocess the data to make it usable.\n",
    "2) Train first model.\n",
    "3) We will find which features are most likely to contribute the most to predictions.\n",
    "4) Train final model.\n",
    "5) Save model to file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "111c7464-cda7-4a7f-beb9-8f142c9df761",
   "metadata": {},
   "source": [
    "First we load up the data from the IMF_DATA file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ecd8f5af-aec7-4a9b-993c-8db61daff9fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "imf_data = pd.read_pickle(\"IMF_DATA.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e0818f0-ca8a-4243-8053-24a14d76fd98",
   "metadata": {},
   "source": [
    "Now we filter out the data we will use and split it to other_data and gdp_data\\\n",
    "**Note: estimates will not be used**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "92681ebc-fe6b-4743-a70a-1ac17c63b20c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_predictions(row):\n",
    "    DATA_END_DATE = 2025\n",
    "    DATA_START_DATE = 1980\n",
    "    \n",
    "    prediction_start_year = int(row['Estimates Start After'])\n",
    "    \n",
    "    if prediction_start_year < DATA_START_DATE:\n",
    "        row.loc[DATA_START_DATE : DATA_END_DATE] = 0\n",
    "    elif prediction_start_year < DATA_END_DATE:\n",
    "        row.loc[prediction_start_year + 1 : DATA_END_DATE] = 0\n",
    "        \n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1bfe3725-f962-41ed-a872-77b4549063a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "imf_data = imf_data.apply(remove_predictions, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "681de216-3dc8-4716-a8d7-fd5eb9b88b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetching the GDP per capita data denoted in US dollars\n",
    "gdp_pattern = r'\\bGross domestic product per capita.*'\n",
    "gdp_data = imf_data[imf_data['Subject Descriptor'].str.contains(gdp_pattern)]\n",
    "gdp_data = gdp_data[gdp_data['Units'].str.contains(r'U.S. dollars')]\n",
    "\n",
    "# Dropping all other columns apart from 'Country' and 1980 : 2025\n",
    "columns_to_be_dropped = ['WEO Country Code', 'ISO', 'WEO Subject Code', 'Subject Descriptor', 'Subject Notes',\n",
    "                               'Units', 'Scale', 'Country/Series-specific Notes', 'Estimates Start After']\n",
    "gdp_data.drop(columns_to_be_dropped, axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "55ca917d-2980-4aaf-b7a9-cfee010f5bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping rows that contain GDP related data\n",
    "gdp_pattern = r'\\bGross domestic product.*'\n",
    "other_data = imf_data[imf_data['Subject Descriptor'].str.contains(gdp_pattern) == False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "025a728e-a13b-4ed0-bc90-7d759757be27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keeping only Units that i will use, as there are some Subject descriptors that have over 195 values\n",
    "# r'Index', r'U.S. dollars', \n",
    "units_to_be_dropped = [r'Missing']\n",
    "\n",
    "# Looping over units_to_be_dropped and dropping the rows that contain the expression\n",
    "for expression in units_to_be_dropped:\n",
    "    other_data = other_data[other_data['Units'].str.contains(expression) == False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ec66495f-1696-4aa7-93ab-5e8e3272dc5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making a human readable subjects dataframe where one columns is the WEO subject code the other is subject descriptor\n",
    "subjects = other_data.loc[:, ['Subject Descriptor', 'WEO Subject Code', 'Units']]\n",
    "subjects.drop_duplicates(inplace = True) \n",
    "\n",
    "# Uncomment code if you want to see the human readable subjects dataframe\n",
    "#print(subjects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1bbecbef-c7e1-4aff-8ab9-8898edcbd65a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping columns in other_data that will be of no use\n",
    "columns_to_be_dropped = ['Subject Descriptor', 'WEO Country Code', 'ISO', \n",
    "                         'Subject Notes', 'Country/Series-specific Notes', 'Estimates Start After', 'Units', 'Scale']\n",
    "other_data.drop(columns_to_be_dropped, axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a6d24c3-0a53-4b48-99a1-d2ddf51367c8",
   "metadata": {},
   "source": [
    "Now that we have all the subjects in the units we need, we can start converting both dataframes from wide to long versions and also drop zero rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8655e506-b7af-4734-96ad-e1e5f4a609f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Melting other_data to long format so the year can become a column\n",
    "other_data = other_data.melt(id_vars=['Country', 'WEO Subject Code'], var_name='Year', value_name='Value')\n",
    "\n",
    "# Pivoting other_data to make every subject code a seperate column and the resetting the index\n",
    "other_data = other_data.pivot_table(index=['Country', 'Year'], columns='WEO Subject Code', values='Value').reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1907d27a-110e-4069-88d3-8cbab9715012",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Melting the gdp_data so I can merge based on 'Country' and 'Year' columns\n",
    "gdp_data = gdp_data.melt(id_vars='Country', var_name='Year', value_name='GDP per capita')\n",
    "\n",
    "# Merging the data on 'Country' and 'Year' columns\n",
    "merged_data = other_data.merge(gdp_data, on = ['Country', 'Year'], how = 'inner')\n",
    "\n",
    "# Discarding rows if GDP per capita is == 0\n",
    "merged_data = merged_data[merged_data['GDP per capita'] != 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd72cbab-6951-4727-96fd-d70e6a9b4ef2",
   "metadata": {},
   "source": [
    "Now that we have a complete data frame where there is no null values for GDP per capita we can start searching for features we will use.\n",
    "First we will need to find which columns have the least nan values (0 in this case). There is 6500 rows that have GDP per capita values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e3ff129c-5809-42de-8807-1b886a25c4f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We find which columns have the least zero values\n",
    "non_zero_values = {}\n",
    "for column in merged_data.columns:\n",
    "    non_zero_values[column] = len(merged_data[merged_data[column] != 0])\n",
    "\n",
    "#pd.DataFrame(list(non_zero_values.items()), columns = ['Column', 'Number']\n",
    "#            ).sort_values('Number', axis = 0, ascending = False\n",
    "#                        ).merge(subjects, how='inner', left_on = 'Column', right_on = 'WEO Subject Code')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6eb762a-1cf4-4095-93b7-14f4d9f177d7",
   "metadata": {},
   "source": [
    "Now we can finally try to train our first model (XGBRegressor) with all the features. First we still have to impute our data with the help of a pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1c453c74-7566-4ed3-8ff8-b4763257aee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replacing 0 values with nan \n",
    "merged_data.replace({0 : float('nan')}, inplace = True)\n",
    "\n",
    "# Imputing nan values \n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "y = merged_data['GDP per capita']\n",
    "X = merged_data.drop(['GDP per capita', 'Country', 'Year'], axis=1)\n",
    "\n",
    "# Divide data into training and validation subsets\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, train_size=0.8, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d8c984c0-de46-4cb9-a23e-ccce07cb4a7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE: 1234.8449853572529\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "train_preprocessor = SimpleImputer()\n",
    "\n",
    "train_model = XGBRegressor(n_estimators=750, learning_rate=0.1)\n",
    "\n",
    "train_pipeline = Pipeline(steps=[('preprocessor', train_preprocessor),\n",
    "                              ('model', train_model)\n",
    "                             ])\n",
    "\n",
    "# Preprocessing of training data, fit model \n",
    "train_pipeline.fit(X_train, y_train,)\n",
    "\n",
    "# Preprocessing of validation data, get predictions\n",
    "preds = train_pipeline.predict(X_valid)\n",
    "\n",
    "# Evaluate the model\n",
    "score = mean_absolute_error(y_valid, preds)\n",
    "print('MAE:', score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9caf12a0-1ab2-4ac7-baf6-ff5167db7c98",
   "metadata": {},
   "source": [
    "Features used for training first model: 'BCA', 'BCA_NGDPD', 'FLIBOR6', 'GGR', 'GGR_NGDP',\n",
    "       'GGSB', 'GGSB_NPGDP', 'GGX', 'GGXCNL', 'GGXCNL_NGDP', 'GGXONLB',\n",
    "       'GGXONLB_NGDP', 'GGXWDG', 'GGXWDG_NGDP', 'GGXWDN', 'GGXWDN_NGDP',\n",
    "       'GGX_NGDP', 'LE', 'LP', 'LUR', 'NGAP_NPGDP', 'NGSD_NGDP', 'NID_NGDP',\n",
    "       'PCPI', 'PCPIE', 'PCPIEPCH', 'PCPIPCH', 'PPPEX', 'TMG_RPCH', 'TM_RPCH',\n",
    "       'TXG_RPCH', 'TX_RPCH'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9587e941-cbc2-4c61-8422-57f2a6845a50",
   "metadata": {},
   "source": [
    "Now using mi scores we will find the top 5 most important features for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e5ef86ee-4eb8-4d8a-bbbf-0d2b8864051f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LE          0.344354\n",
       "BCA         0.302798\n",
       "GGR_NGDP    0.276084\n",
       "LUR         0.273461\n",
       "LP          0.239094\n",
       "GGX_NGDP    0.227638\n",
       "PPPEX       0.222303\n",
       "Name: MI Scores, dtype: float64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_selection import mutual_info_regression\n",
    "\n",
    "# Function for scoring features, it doens't except nan values :(\n",
    "def make_mi_scores(X, y):\n",
    "    mi_scores = mutual_info_regression(X, y)\n",
    "    mi_scores = pd.Series(mi_scores, name=\"MI Scores\", index=X.columns)\n",
    "    mi_scores = mi_scores.sort_values(ascending=False)\n",
    "    return mi_scores\n",
    "\n",
    "mi_scores = make_mi_scores(X.fillna(0), y.fillna(0))\n",
    "mi_scores.head(7)  # show a few features with their MI scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07b3933d-0f76-44e4-82ca-9ca1dd6ab072",
   "metadata": {},
   "source": [
    "Features we will use: LE, BCA, GGR_NGDP, LUR, LP\\\n",
    "LE - Employment, BCA - Current account balance, GGR_NGDP - General government revenue, LUR - Unemployment rate, LP - Population.\n",
    "I will also add the 3 features that will be needed in the nex task for training (We will need, continent, population)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "011a462e-840e-4e0e-967f-dad60703ea7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE: 1967.8464338720576\n"
     ]
    }
   ],
   "source": [
    "features_final = ['LE', 'BCA', 'GGR_NGDP', 'LUR', 'LP']\n",
    "\n",
    "X_final = X[features_final]\n",
    "\n",
    "# Divide data into training and validation subsets\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_final, y, train_size=0.8, test_size=0.2, random_state=69)\n",
    "\n",
    "train_pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Preprocessing of validation data, get predictions\n",
    "preds = train_pipeline.predict(X_valid)\n",
    "\n",
    "# Evaluate the model\n",
    "score = mean_absolute_error(y_valid, preds)\n",
    "print('MAE:', score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d8f9c6f-07ab-48b1-b082-be9f85830056",
   "metadata": {},
   "source": [
    "As you can see by the MAE the second model is a bit worse than the first (by about 750 USD), though we could improve on this further by making custom features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20fff6e2-b41f-4690-bc82-d4b7797c72a9",
   "metadata": {},
   "source": [
    "Now we will save the last model to a file for our next task, I will be using joblib for this as it is generally more efficient for dealing with large NumPy arrays.\n",
    "\n",
    "**Note: because our model is part of a pipeline we will save the pipeline itself**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "990a8dfd-4186-45f9-80b1-8754418568f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['finalized_pipeline.sav']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "filename = 'finalized_pipeline.sav'\n",
    "joblib.dump(train_pipeline, filename)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
